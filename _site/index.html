<!DOCTYPE html>

<html lang="en">

  <head>
  <meta charset="UTF-8">
  <title>Learnable Triangulation of Human Pose</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="/learnable-triangulation-of-human-pose/css/normalize.css">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="/learnable-triangulation-of-human-pose/css/cayman.css">

  <!-- favicon support -->
  <!-- Begin Jekyll Favicon tag v0.2.6 -->
<!-- Classic -->
<link rel="shortcut icon" href="/learnable-triangulation-of-human-pose/favicon.ico">
<link rel="icon" sizes="48x48 32x32 16x16" href="/learnable-triangulation-of-human-pose/favicon.ico">
  <link rel="icon" sizes="16x16" type="image/png" href="/learnable-triangulation-of-human-pose/assets/images/favicon-16x16.png">
  <link rel="icon" sizes="32x32" type="image/png" href="/learnable-triangulation-of-human-pose/assets/images/favicon-32x32.png">
  <link rel="icon" sizes="64x64" type="image/png" href="/learnable-triangulation-of-human-pose/assets/images/favicon-64x64.png">
  <link rel="icon" sizes="144x144" type="image/png" href="/learnable-triangulation-of-human-pose/assets/images/favicon-144x144.png">
<!-- Safari -->
  <link rel="apple-touch-icon" sizes="57x57" href="/learnable-triangulation-of-human-pose/assets/images/favicon-57x57.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/learnable-triangulation-of-human-pose/assets/images/favicon-76x76.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/learnable-triangulation-of-human-pose/assets/images/favicon-120x120.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/learnable-triangulation-of-human-pose/assets/images/favicon-152x152.png">
  <link rel="apple-touch-icon" sizes="167x167" href="/learnable-triangulation-of-human-pose/assets/images/favicon-167x167.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/learnable-triangulation-of-human-pose/assets/images/favicon-180x180.png">

<link rel="mask-icon" color="#ffffff" href="/learnable-triangulation-of-human-pose/assets/images/safari-pinned-tab.svg">
<!-- Chrome -->
  <link rel="icon" sizes="192x192" href="/learnable-triangulation-of-human-pose/assets/images/favicon-192x192.png">
  <link rel="icon" sizes="96x96" href="/learnable-triangulation-of-human-pose/assets/images/favicon-96x96.png">
  <link rel="icon" sizes="48x48" href="/learnable-triangulation-of-human-pose/assets/images/favicon-48x48.png">
<link rel="manifest" href="/learnable-triangulation-of-human-pose/manifest.webmanifest">
<!-- IE -->
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="/learnable-triangulation-of-human-pose/assets/images/favicon-144x144.png">
<meta name='msapplication-config' content="/learnable-triangulation-of-human-pose/browserconfig.xml">
<!-- End Jekyll Favicon tag -->

  <!-- Mathjax Support -->
  <script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
</head>


  <body>
    <section class="page-header">
  <h1 class="project-name">Learnable Triangulation of Human Pose</h1>
  <h2 class="project-tagline">Karim Iskakov, Egor Burkov, Victor Lempitsky, Yury Malkov</h2>
  <a href="#" class="btn">arXiv (soon)</a>
  <a href="https://www.youtube.com/watch?v=z3f3aPSuhqg" class="btn">Video</a>
  <a href="#" class="btn">Code (soon)</a>
  <a href="#bibtex" class="btn">BibTeX (soon)</a>
</section>


    <section class="main-content">

      <h1 id="abstract">Abstract</h1>
<p>We present two novel solutions for multi-view 3D human pose estimation based on new learnable triangulation methods that combine 3D information from multiple 2D views.</p>

<p>The <a href="#algebraic">first (baseline) solution</a> is a basic differentiable algebraic triangulation with an addition of confidence weights estimated from the input images.
<a href="#volumetric">The second</a>, more complex, solution is based on <a href="#unprojection">volumetric aggregation</a> of 2D feature maps from the 2D backbone followed by refinement via 3D convolutions that produce final 3D joint heatmaps.</p>

<p>Crucially, both of the approaches are end-to-end differentiable, which allows us to directly optimize the target metric. We demonstrate <a href="#transfer-from-panoptic-cmu-to-human36m">transferability</a> of the solutions across datasets and <a href="#human3-6m">considerably improve</a> the multi-view state of the art on the Human3.6M dataset.</p>

<div class="youtube-responsive-container">
  <iframe width="560" height="315" src="https://www.youtube.com/embed/z3f3aPSuhqg?controls=1" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</div>

<h1 id="models">Models</h1>
<p>Our approaches assumes we have synchronized video streams from <script type="math/tex">C</script> cameras with known projection matrices <script type="math/tex">P_c</script> capturing performance of a single person in the scene. We aim at estimating the global 3D positions <script type="math/tex">\boldsymbol{y}_{j,t}</script> of a fixed set of human joints with indices <script type="math/tex">j\in(1..J)</script>.</p>

<h2 id="algebraic">Algebraic</h2>
<p>Our first approach is based on algebraic triangulation with learned confidences.</p>

<p><img src="static/algebraic-model.svg" alt="Algebraic model" /></p>

<ol>
  <li>
    <p>2D backbone produces the joints’ heatmaps <script type="math/tex">H_{c,j}</script> and camera-joint confidences <script type="math/tex">w_{c,j}</script>.</p>
  </li>
  <li>
    <p>The 2D positions of the joints <script type="math/tex">\boldsymbol{x}_{c,j}</script> are inferred from 2D joint heatmaps <script type="math/tex">H_{c,j}</script> by applying soft-argmax (with inverse temperature parameter <script type="math/tex">\alpha</script>):</p>

    <script type="math/tex; mode=display">H'_{c,j} = {\text{exp}(\alpha H_{c,j})} / \Big(\sum_{r_x=1}^{W} \sum_{r_y=1}^{H} \text{exp}(\alpha H_{c,j}(\boldsymbol{r})) \Big)</script>

    <script type="math/tex; mode=display">\boldsymbol{x}_{c,j}= \sum\limits_{r_x=1}^{W}{\sum\limits_{r_y=1}^{H} \boldsymbol{r}\cdot( {H'}_{c,j}(\boldsymbol{r})})</script>
  </li>
  <li>
    <p>The 2D positions <script type="math/tex">\boldsymbol{x}_{c,j}</script> together with the confidences <script type="math/tex">w_{c,j}</script> are passed to the algebraic triangulation module which solves triangulation problem in the form of system of weighted linear equations:</p>

    <script type="math/tex; mode=display">(\boldsymbol{w}_j\circ{A_j})\tilde{\boldsymbol{y}}_j=0,</script>

    <p>where <script type="math/tex">\boldsymbol{w_j}</script> - vector of confidences for joint <script type="math/tex">j</script>, <script type="math/tex">A_j</script> - matrix combined of 2D joint coordinates and camera parameters (see details in <a href="#references">[1]</a>) and <script type="math/tex">\tilde{\boldsymbol{y}}_j</script> - target 3D position of joint <script type="math/tex">j</script>.</p>
  </li>
</ol>

<p>All blocks allow backpropagation of the gradients, so the model can be trained end-to-end.</p>

<h2 id="volumetric">Volumetric</h2>
<p>Our second approach is based on volumetric triangulation.</p>

<p><img src="static/volumetric-model.svg" alt="Volumetric model" /></p>

<!-- <div style="display: flex; justify-content: center;">
  <img src="static/volumetric-model.png" alt="Volumetric model" />
</div> -->

<ol>
  <li>
    <p>The 2D backbone produces intermediate feature maps <script type="math/tex">M_{c,k}</script> (note, that unlike the first model, feature maps doesn’t have to be interpretable).</p>
  </li>
  <li>
    <p>Then feature maps are unprojected into a volume <script type="math/tex">V_{c,k}</script> with a per-view aggregation (see animation below):</p>

    <script type="math/tex; mode=display">V_c^{\text{proj}}=P_c V^{\text{coords}}</script>

    <script type="math/tex; mode=display">V_{c,k}^{\text{view}}= M_{c,k}\{V_c^{\text{proj}}\},</script>

    <p>where <script type="math/tex">V_c^{\text{coords}}</script> - absolute coordinates of each voxel, <script type="math/tex">P_c</script> - projection matrix of camera <script type="math/tex">c</script>. Operation <script type="math/tex">\{\cdot\}</script> denotes bilinear sampling.</p>
  </li>
  <li>
    <p>The volume is passed to a 3D convolutional neural network that outputs the interpretable 3D heatmaps <script type="math/tex">V_{j}^{\text{output}}</script>.</p>
  </li>
  <li>
    <p>The output 3D positions <script type="math/tex">\boldsymbol{y}_{j}</script> of the joints are inferred from 3D joint heatmaps by computing soft-argmax:</p>
  </li>
</ol>

<script type="math/tex; mode=display">{V'}_{j}^{\text{output}}={\text{exp}( {V}_{j}^{\text{output}} )} /
      \Big({\sum\limits_{r_x=1}^{W}{\sum\limits_{r_y=1}^{H} \sum\limits_{r_z=1}^{D} \text{exp}( {V}_{j}^{\text{output}}(\boldsymbol{r})) \Big)}}</script>

<script type="math/tex; mode=display">\boldsymbol{y}_{j} = \sum\limits_{r_x=1}^{W} \sum\limits_{r_y=1}^{H} \sum\limits_{r_z=1}^{D} \boldsymbol{r} \cdot {V'}_{j}^{\text{output}}(\boldsymbol{r})</script>

<p>Volumetric model is also fully differentiable and can be trained end-to-end.</p>

<h3 id="unprojection">Unprojection</h3>
<p>Here’s an animation showing how unprojection works for 2 cameras:</p>

<p><img src="static/unprojection.gif" alt="Algebraic model" /></p>

<h1 id="results">Results</h1>
<p>We conduct experiments on two available large multi-view datasets with available ground-truth 3D pose annotations: Human3.6M <a href="#references">[2]</a> and CMU Panoptic <a href="#references">[3]</a>.</p>

<h2 id="human36m">Human3.6M</h2>
<p>The Human3.6M <a href="#references">[2]</a> is currently one of the largest 3D human pose benchmarks with many reported results both for monocular and multi-view setups. The full dataset consist of 3.6 million frames from 4 synchronized 50 Hz digital cameras along with the 3D pose annotations (collected using a 10 separate IR-cameras marker-based MoCap system).</p>

<p>Here and further we report only summary of our results. Please refer to our paper for more details.</p>

<h3 id="mpjpe-relative-to-pelvis-mm">MPJPE relative to pelvis, mm:</h3>

<table>
  <thead>
    <tr>
      <th> </th>
      <th style="text-align: center">MPJPE (averaged across all actions)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Tome et al. <a href="#references">[4]</a></td>
      <td style="text-align: center">52.8</td>
    </tr>
    <tr>
      <td>Kadkhodamohammadi &amp; Padoy <a href="#references">[5]</a></td>
      <td style="text-align: center">49.1</td>
    </tr>
    <tr>
      <td>RANSAC (our implementation)</td>
      <td style="text-align: center">27.4</td>
    </tr>
    <tr>
      <td><strong>Ours algebraic</strong></td>
      <td style="text-align: center">22.6</td>
    </tr>
    <tr>
      <td><strong>Ours volumetric</strong></td>
      <td style="text-align: center"><strong>20.8</strong></td>
    </tr>
  </tbody>
</table>

<h3 id="mpjpe-absolute-mm-filtered-scenes-with-erroneous-ground-truth-3d-pose-annotations">MPJPE absolute, mm (filtered scenes with erroneous ground-truth 3D pose annotations):</h3>

<table>
  <thead>
    <tr>
      <th> </th>
      <th style="text-align: center">MPJPE (averaged across all actions)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>RANSAC (our implementation)</td>
      <td style="text-align: center">22.8</td>
    </tr>
    <tr>
      <td><strong>Ours algebraic</strong></td>
      <td style="text-align: center">19.2</td>
    </tr>
    <tr>
      <td><strong>Ours volumetric</strong></td>
      <td style="text-align: center"><strong>17.7</strong></td>
    </tr>
  </tbody>
</table>

<h3 id="mpjpe-relative-to-pelvis-mm-monocular-methods">MPJPE relative to pelvis, mm (monocular methods):</h3>

<table>
  <thead>
    <tr>
      <th> </th>
      <th style="text-align: center">MPJPE (averaged across all actions)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Sun et al. <a href="#references">[6]</a></td>
      <td style="text-align: center">49.6</td>
    </tr>
    <tr>
      <td>Pavllo et al. <a href="#references">[7]</a></td>
      <td style="text-align: center"><strong>46.8</strong></td>
    </tr>
    <tr>
      <td><strong>Ours volumetric single view</strong></td>
      <td style="text-align: center">49.9</td>
    </tr>
  </tbody>
</table>

<h2 id="cmu-panoptic">CMU Panoptic</h2>
<p>The CMU Panoptic <a href="#references">[3]</a> is a new multi-camera dataset maintained by the Carnegie Mellon University. The dataset provides 30 Hz Full-HD videostreams of 40 subjects from up to 31 synchronized cameras.</p>

<h3 id="mpjpe-relative-to-pelvis-mm-4-cameras">MPJPE relative to pelvis, mm [4 cameras]:</h3>

<table>
  <thead>
    <tr>
      <th> </th>
      <th style="text-align: center">MPJPE (averaged across all actions)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>RANSAC (our implementation)</td>
      <td style="text-align: center">39.5</td>
    </tr>
    <tr>
      <td><strong>Ours algebraic</strong></td>
      <td style="text-align: center">21.3</td>
    </tr>
    <tr>
      <td><strong>Ours volumetric</strong></td>
      <td style="text-align: center"><strong>13.7</strong></td>
    </tr>
  </tbody>
</table>

<p>Illustration of the difference in performance of the approaches on the CMU dataset validation (using 2 cameras) that demonstrates the robustness of the volumetric triangulation approach:</p>

<div style="display: flex; justify-content: center;">
  <img src="static/cmu-panoptic-results.png" alt="CMU Panoptic results" />
</div>

<p>Estimate of the MPJPE absolute error on the subset of CMU validation versus the numbers of cameras (up to 28, treating the annotations from CMU as ground truth):</p>

<p><img src="static/cmu-panoptic-camera-plot.png" alt="CMU Panoptic camera plot" /></p>

<h2 id="transfer-from-panoptic-cmu-to-human36m">Transfer from Panoptic CMU to Human3.6M</h2>
<p>We also conducted experiments to demonstrate that the learnt model is indeed generalizes to new setups. For that we applied a CMU-trained model to Human3.6M validation scenes. Below you can see example image (see video for more demonstrations):</p>

<div style="display: flex; justify-content: center;">
  <img src="static/transfer-results.png" alt="Transfer results" />
</div>

<h1 id="bibtex">BibTeX</h1>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>TBA
</code></pre></div></div>

<h1 id="references">References</h1>
<ul>
  <li><a href="#references">[1]</a> R. Hartley and A. Zisserman. <strong>Multiple view geometry in computer vision</strong>.</li>
  <li><a href="#references">[2]</a> C. Ionescu, D. Papava, V. Olaru, and C. Sminchisescu. <strong>Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</strong>.</li>
  <li><a href="#references">[3]</a> H. Joo, T. Simon, X. Li, H. Liu, L. Tan, L. Gui, S. Banerjee, T.  S. Godisart, B. Nabbe, I. Matthews, T. Kanade,S. Nobuhara, and Y. Sheikh. <strong>Panoptic studio: A massively multiview system for social interaction capture</strong>.</li>
  <li><a href="#references">[4]</a> D. Tome, M. Toso, L. Agapito, and C. Russell.  <strong>Rethinking Pose in 3D: Multi-stage Refinement and Recovery for Markerless Motion Capture</strong>.</li>
  <li><a href="#references">[5]</a> A. Kadkhodamohammadi and N. Padoy. <strong>A generalizable approach for multi-view 3D human pose regression</strong>.</li>
  <li><a href="#references">[6]</a> X. Sun, B. Xiao, S. Liang, and Y. Wei. <strong>Integral human pose regression</strong>.</li>
  <li><a href="#references">[7]</a> D. Pavllo, C. Feichtenhofer, D. Grangier, and M. Auli. <strong>3d human pose estimation in video with temporal convolutions and semi-supervised training</strong>.</li>
</ul>


      <footer class="site-footer">
  <span class="site-footer-owner"><a href="http://localhost:4000">Learnable Triangulation of Human Pose</a> is maintained by <a href="http://saic-violet.github.io/learnable-triangulation-of-human-pose">Karim Iskakov</a>.</span>
  <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
</footer>


    </section>

  </body>
</html>
